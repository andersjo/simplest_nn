{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Feed-forward neural networks \n",
    "\n",
    "Neural networks are different things to different people. \n",
    "\n",
    "Some common and useful perspectives on neural networks (NN) are listed below:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* **Linear algebra view**. An NN is a series of non-linear maps. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* **Perceptron view**. An NN is a network of computational neurons. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* **Computational graph view**. An NN is computational procedure involving a loss and updatable parameters (often with automatic differentiation). \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* **Machine learning view**. An NN represents a non-linear prediction function $f(\\mathbf{x}; \\mathbf{W})$ with complicated internal structure. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Topics for today\n",
    "\n",
    "* Why do we need non-linearieties?\n",
    "* Forward and backward as instances of dynamic programming.\n",
    "* Differentiation is a linear approximation.\n",
    "* Gradient checking."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll start with a hypothetical (xkcd-style).\n",
    "\n",
    "<img src=\"https://what-if.xkcd.com/imgs/whatif-logo.png\">\n",
    "\n",
    "### What if all the non-linearities in an NN suddenly vanished?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "A neural network with an input layer, a middle layer, and an output layer computes the following:\n",
    "\n",
    "$$\\mathbf{y} = g(W^{(0)}g(W^{(1)}g(W^{(0)}\\mathbf{x})))$$\n",
    "\n",
    "$g$ is a non-linearity, which could be different for each layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "If we change $g$ to a linear function (e.g. a scaling factor), it can simply be multiplied into the weights matrices. Below we assume that $g = 1$, which allows us to simplify the expression:\n",
    "\n",
    "$$\\mathbf{y} = (W^{(0)}(W^{(1)}(W^{(0)}\\mathbf{x})))$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Since matrix multiplication is associative:\n",
    "\n",
    "$$A(BC) = (AB)C,$$\n",
    "\n",
    "we can get rid of the brackets altogether:\n",
    "\n",
    "$$\\mathbf{y} = W^{(0)}W^{(1)}W^{(0)}\\mathbf{x}.$$\n",
    "\n",
    "The series of linear transformations can be summarized in a single transformation matrix :\n",
    "\n",
    "$$T = W^{(0)}W^{(1)}W^{(0)}.$$\n",
    "\n",
    "And so the prediction of the neural network becomes:\n",
    "\n",
    "$$\\mathbf{y} = T\\mathbf{x}.$$\n",
    "\n",
    "The effective number of parameters in the now non non-linear neural network is $|\\mathbf{y}| \\times |\\mathbf{x}|$, which is precisely the same as a standard linear model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Toy network\n",
    "\n",
    "Image of toy network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forward pass (predict)\n",
    "\n",
    "### Output as a function of the input \n",
    "\n",
    "Using the NN structure as a guide, we can write down an expression to calculate the output of the network in terms of the output. \n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "y = g_y\\Big(& g_s\\big(g_r(x_0 (x_0 \\to r_0) + x_1 (x_1 \\to r_0)) (r_0 \\to s_0) + g_r(x_0 (x_0 \\to r_1) + x_1 (x_1 \\to r_1) (r_1 \\to s_0)\\big) (s_0 \\to y) +\\\\& g_s\\big(g_r(x_0 (x_0 \\to r_0) + x_1 (x_1 \\to r_0)) (r_0 \\to s_1) + g_r(x_0 (x_0 \\to r_1) + x_1 (x_1 \\to r_1)) (r_1 \\to s_1)\\big) (s_1 \\to y)\\Big)\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "The equation avoids any explicit mentioning of NN concepts such as layer and node. Arguably, this is a silly thing to do. However, it serves a purpose here, namely to demonstrate why these are useful.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Massive duplication of effort\n",
    "\n",
    "The input-to-output equation has many repeatet calculations, which are marked in blue below:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "y = g_y \\Big(& g_s \\big( \\color{blue}{g_r(x_0 W_{x_0, r_0} + x_1 W_{x_1, r_0})} W_{r_0, s_0} + \\color{blue}{g_r(x_0 W_{x_0, r_1} + x_1 W_{x_1, r_1})} W_{r_1, s_0} \\big) W_{s_0, y} +\\\\& g_s \\big( \\color{blue}{g_r(x_0 W_{x_0, r_0} + x_1 W_{x_1, r_0})} W_{r_0, s_1} + \\color{blue}{g_r(x_0 W_{x_0, r_1} + x_1 W_{x_1, r_1})} W_{r_1, s_1} \\big) W_{s_1, y} \\Big)\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Expresing the output as a set of node-centered equations avoids duplication.\n",
    "\n",
    "$$\n",
    "r_0 = g_r(x_0 W_{x_0, r_0} + x_1 W_{x_1, r_0})\\\\\n",
    "r_1 = g_r(x_0 W_{x_0, r_1} + x_1 W_{x_1, r_1})\\\\\n",
    "s_0 = g_s(r_0 W_{r_0, s_0} + r_1 W_{r_1, s_0})\\\\\n",
    "s_1 = g_s(r_0 W_{r_0, s_1} + r_1 W_{r_1, s_1})\\\\\n",
    "y = g_y(s_0 W_{s_0, y} + s_1 W_{s_1, y})\n",
    "$$\n",
    "\n",
    "The forward pass (**predict**) in a neural network thus uses dynamic programming, although in a simple form.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backward pass (derivates)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "High level idea: Calculate partial derivatives with respect to each parameter in the network. Again, this can be done inefficiently with duplication of effort as demonstrated below:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{align}\n",
    "%\\frac{\\partial f}{\\partial w(s_0 \\to y)} = & f'(s_0 \\to y)\\\\\n",
    "%\\frac{\\partial f}{\\partial w(s_1 \\to y)} = & f'(s_1 \\to y)\\\\\n",
    "\\frac{\\partial f}{\\partial (r_0 \\to s_0)} = & \\frac{\\partial f}{\\partial (s_0 \\to y)} + r_0 \\frac{\\partial g_s}{\\partial (r_0 \\to s_0)}\\\\\n",
    "\\frac{\\partial f}{\\partial (r_0 \\to s_1)} = & \\frac{\\partial f}{\\partial (s_1 \\to y)} + r_0 \\frac{\\partial g_s}{\\partial (r_0 \\to s_1)}\\\\\n",
    "\\frac{\\partial f}{\\partial (r_1 \\to s_0)} = & \\frac{\\partial f}{\\partial (s_0 \\to y)} + r_0 \\frac{\\partial g_s}{\\partial (r_1 \\to s_0)}\\\\\n",
    "\\frac{\\partial f}{\\partial (r_1 \\to s_1)} = & \\frac{\\partial f}{\\partial (s_0 \\to y)} + r_0 \\frac{\\partial g_s}{\\partial (r_1 \\to s_1)}\\\\\n",
    "\\frac{\\partial f}{\\partial (x_0 \\to r_0)} = & x_0 \\frac{\\partial g_r}{\\partial (x_0 \\to r_0)} \\Bigg( \\color{blue}{\\Big( \\frac{\\partial f}{\\partial (s_0 \\to y)} + r_0 \\frac{\\partial g_s}{\\partial (r_0 \\to s_0)} \\Big)} + \\color{blue}{\\Big( \\frac{\\partial f}{\\partial (s_1 \\to y)} + r_0 \\frac{\\partial g_s}{\\partial (r_0 \\to s_1)} \\Big)} \\Bigg)\\\\\n",
    "\\frac{\\partial f}{\\partial (x_0 \\to r_1)} = & x_0 \\frac{\\partial g_r}{\\partial (x_0 \\to r_1)} \\Bigg( \\color{blue}{\\Big( \\frac{\\partial f}{\\partial (s_0 \\to y)} + r_1 \\frac{\\partial g_s}{\\partial (r_1 \\to s_0)} \\Big)} + \\color{blue}{\\Big( \\frac{\\partial f}{\\partial (s_1 \\to y)} + r_1 \\frac{\\partial g_s}{\\partial (r_1 \\to s_1)} \\Big)} \\Bigg)\\\\\n",
    "\\frac{\\partial f}{\\partial (x_1 \\to r_0)} = & x_1 \\frac{\\partial g_r}{\\partial (x_1 \\to r_0)} \\Bigg( \\color{blue}{\\Big( \\frac{\\partial f}{\\partial (s_0 \\to y)} + r_0 \\frac{\\partial g_s}{\\partial (r_0 \\to s_0)} \\Big)} + \\color{blue}{\\Big( \\frac{\\partial f}{\\partial (s_1 \\to y)} + r_0 \\frac{\\partial g_s}{\\partial (r_0 \\to s_1)} \\Big)} \\Bigg)\\\\\n",
    "\\frac{\\partial f}{\\partial (x_1 \\to r_1)} = & x_1 \\frac{\\partial g_r}{\\partial (x_1 \\to r_1)} \\Bigg( \\color{blue}{\\Big( \\frac{\\partial f}{\\partial (s_0 \\to y)} + r_1 \\frac{\\partial g_s}{\\partial (r_1 \\to s_0)} \\Big)} + \\color{blue}{\\Big( \\frac{\\partial f}{\\partial (s_1 \\to y)} + r_1 \\frac{\\partial g_s}{\\partial (r_1 \\to s_1)} \\Big)} \\Bigg)\\\\\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
